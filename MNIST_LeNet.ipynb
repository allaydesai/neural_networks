{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST LeNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All modules imported.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import hashlib\n",
    "import os\n",
    "import pickle\n",
    "from urllib.request import urlretrieve\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.utils import resample\n",
    "from sklearn.utils import shuffle\n",
    "from tqdm import tqdm\n",
    "from zipfile import ZipFile\n",
    "import matplotlib.pyplot as plt\n",
    "print('All modules imported.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files downloaded.\n"
     ]
    }
   ],
   "source": [
    "def download(url, file):\n",
    "    \"\"\"\n",
    "    Download file from <url>\n",
    "    :param url: URL to file\n",
    "    :param file: Local file path\n",
    "    \"\"\"\n",
    "    if not os.path.isfile(file):\n",
    "        print('Downloading ' + file + '...')\n",
    "        urlretrieve(url, file)\n",
    "        print('Download Finished')\n",
    "\n",
    "# Download the training and test dataset.\n",
    "download('https://s3.amazonaws.com/udacity-sdc/notMNIST_train.zip', 'notMNIST_train.zip')\n",
    "download('https://s3.amazonaws.com/udacity-sdc/notMNIST_test.zip', 'notMNIST_test.zip')\n",
    "\n",
    "# Make sure the files aren't corrupted\n",
    "assert hashlib.md5(open('notMNIST_train.zip', 'rb').read()).hexdigest() == 'c8673b3f28f489e9cdf3a3d74e2ac8fa',\\\n",
    "        'notMNIST_train.zip file is corrupted.  Remove the file and try again.'\n",
    "assert hashlib.md5(open('notMNIST_test.zip', 'rb').read()).hexdigest() == '5d3c7e653e63471c88df796156a9dfa9',\\\n",
    "        'notMNIST_test.zip file is corrupted.  Remove the file and try again.'\n",
    "\n",
    "# Wait until you see that all files have been downloaded.\n",
    "print('All files downloaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 210001/210001 [02:28<00:00, 1413.26files/s]\n",
      "100%|███████████████████████████████████████| 10001/10001 [00:02<00:00, 4018.45files/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All features and labels uncompressed.\n"
     ]
    }
   ],
   "source": [
    "def uncompress_features_labels(file):\n",
    "    \"\"\"\n",
    "    Uncompress features and labels from a zip file\n",
    "    :param file: The zip file to extract the data from\n",
    "    \"\"\"\n",
    "    features = []\n",
    "    labels = []\n",
    "\n",
    "    with ZipFile(file) as zipf:\n",
    "        # Progress Bar\n",
    "        filenames_pbar = tqdm(zipf.namelist(), unit='files')\n",
    "        \n",
    "        # Get features and labels from all files\n",
    "        for filename in filenames_pbar:\n",
    "            # Check if the file is a directory\n",
    "            if not filename.endswith('/'):\n",
    "                with zipf.open(filename) as image_file:\n",
    "                    image = Image.open(image_file)\n",
    "                    image.load()\n",
    "                    # Load image data as 1 dimensional array\n",
    "                    # We're using float32 to save on memory space\n",
    "                    feature = np.array(image, dtype=np.float32).flatten()\n",
    "\n",
    "                # Get the the letter from the filename.  This is the letter of the image.\n",
    "                label = os.path.split(filename)[1][0]\n",
    "\n",
    "                features.append(feature)\n",
    "                labels.append(label)\n",
    "    return np.array(features), np.array(labels)\n",
    "\n",
    "# Get the features and labels from the zip files\n",
    "train_features, train_labels = uncompress_features_labels('notMNIST_train.zip')\n",
    "test_features, test_labels = uncompress_features_labels('notMNIST_test.zip')\n",
    "\n",
    "# Limit the amount of data to work with a docker container\n",
    "docker_size_limit = 150000 #500,000 training images\n",
    "train_features, train_labels = resample(train_features, train_labels, n_samples=docker_size_limit)\n",
    "\n",
    "# Set flags for feature engineering.  This will prevent you from skipping an important step.\n",
    "is_features_normal = False\n",
    "is_labels_encod = False\n",
    "\n",
    "# Wait until you see that all features and labels have been uncompressed.\n",
    "print('All features and labels uncompressed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 210001/210001 [00:49<00:00, 4225.56files/s]\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-89-52a21e2d6027>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0muncompress_features_labels\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'notMNIST_train.zip'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_features\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-2f42beaca622>\u001b[0m in \u001b[0;36muncompress_features_labels\u001b[1;34m(file)\u001b[0m\n\u001b[0;32m     27\u001b[0m                 \u001b[0mfeatures\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeature\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m                 \u001b[0mlabels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;31m# Get the features and labels from the zip files\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_features, train_labels = uncompress_features_labels('notMNIST_train.zip')\n",
    "print(train_features.shape, train_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_features (142500, 784)\n",
      "Training Image (784,)\n",
      "train_labels (142500, 10)\n"
     ]
    }
   ],
   "source": [
    "print(\"train_features\", train_features.shape)\n",
    "print(\"Training Image\", train_features[2].shape)\n",
    "print(\"train_labels\", train_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAFsAAABZCAYAAABR/liSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1569ffd9b00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "index = random.randint(0, len(train_features))\n",
    "image = train_features[index].squeeze()\n",
    "\n",
    "plt.figure(figsize=(1,1))\n",
    "plt.imshow(image.reshape((28,28)), cmap=\"gray\")\n",
    "print(train_labels[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features, train_labels = shuffle(train_features, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize_grayscale(image_data):\n",
    "    \"\"\"\n",
    "    Normalize the image data with Min-Max scaling to a range of [0.1, 0.9]\n",
    "    :param image_data: The image data to be normalized\n",
    "    :return: Normalized image data\n",
    "    \"\"\"\n",
    "    Xmin = np.min(image_data) #0\n",
    "    Xmax = np.max(image_data) #255\n",
    "    a = 0.1\n",
    "    b = 0.9\n",
    "    #print(\"Xmin:\", Xmin, \"Xmax:\", Xmax)\n",
    "    return a + (((image_data-Xmin)*(b-a))/(Xmax- Xmin))\n",
    "\n",
    "if not is_features_normal:\n",
    "    train_features = normalize_grayscale(train_features)\n",
    "    test_features = normalize_grayscale(test_features)\n",
    "    is_features_normal = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One Hot Encode Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if not is_labels_encod:\n",
    "    # Turn labels into numbers and apply One-Hot Encoding\n",
    "    encoder = LabelBinarizer()\n",
    "    encoder.fit(train_labels)\n",
    "    train_labels = encoder.transform(train_labels)\n",
    "    test_labels = encoder.transform(test_labels)\n",
    "    # Change to float32, so it can be multiplied against the features in TensorFlow, which are float32\n",
    "    train_labels = train_labels.astype(np.float32)\n",
    "    test_labels = test_labels.astype(np.float32)\n",
    "    is_labels_encod = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training features and labels randomized and split.\n"
     ]
    }
   ],
   "source": [
    "assert is_features_normal, 'You skipped the step to normalize the features'\n",
    "assert is_labels_encod, 'You skipped the step to One-Hot Encode the labels'\n",
    "\n",
    "# Get randomized datasets for training and validation\n",
    "train_features, valid_features, train_labels, valid_labels = train_test_split(\n",
    "    train_features,\n",
    "    train_labels,\n",
    "    test_size=0.05,\n",
    "    random_state=832289)\n",
    "\n",
    "print('Training features and labels randomized and split.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving data to pickle file...\n",
      "Data cached in pickle file.\n"
     ]
    }
   ],
   "source": [
    "# Save the data for easy access\n",
    "pickle_file = 'notMNIST.pickle'\n",
    "if not os.path.isfile(pickle_file):\n",
    "    print('Saving data to pickle file...')\n",
    "    try:\n",
    "        with open('notMNIST.pickle', 'wb') as pfile:\n",
    "            pickle.dump(\n",
    "                {\n",
    "                    'train_dataset': train_features,\n",
    "                    'train_labels': train_labels,\n",
    "                    'valid_dataset': valid_features,\n",
    "                    'valid_labels': valid_labels,\n",
    "                    'test_dataset': test_features,\n",
    "                    'test_labels': test_labels,\n",
    "                },\n",
    "                pfile, pickle.HIGHEST_PROTOCOL)\n",
    "    except Exception as e:\n",
    "        print('Unable to save data to', pickle_file, ':', e)\n",
    "        raise\n",
    "\n",
    "print('Data cached in pickle file.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data and modules loaded.\n"
     ]
    }
   ],
   "source": [
    "# Load the modules\n",
    "import pickle\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Reload the data\n",
    "pickle_file = 'notMNIST.pickle'\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  pickle_data = pickle.load(f)\n",
    "  train_features = pickle_data['train_dataset']\n",
    "  train_labels = pickle_data['train_labels']\n",
    "  valid_features = pickle_data['valid_dataset']\n",
    "  valid_labels = pickle_data['valid_labels']\n",
    "  test_features = pickle_data['test_dataset']\n",
    "  test_labels = pickle_data['test_labels']\n",
    "  del pickle_data  # Free up memory\n",
    "\n",
    "\n",
    "print('Data and modules loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Stats\n",
      "train_features:  (142500, 784)\n",
      "valid_features:  (7500, 784)\n",
      "test_features:  (10000, 784)\n"
     ]
    }
   ],
   "source": [
    "print(\"Dataset Stats\")\n",
    "print(\"train_features: \", train_features.shape)\n",
    "print(\"valid_features: \", valid_features.shape)\n",
    "print(\"test_features: \", test_features.shape)\n",
    "assert(len(train_features) == len(train_labels))\n",
    "assert(len(test_features) == len(test_labels))\n",
    "assert(len(valid_features) == len(valid_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.00001\n",
    "epochs = 10\n",
    "batch_size = 128\n",
    "\n",
    "# Number of samples to calculate validation and accuracy\n",
    "# Decrease this if you're running out of memory to calculate accuracy\n",
    "test_valid_size = 256\n",
    "\n",
    "# Network Parameters\n",
    "n_classes = 10  # MNIST total classes (0-9 digits)\n",
    "dropout = 0.75  # Dropout, probability to keep units"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weights and Biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store layers weight & bias\n",
    "mean_wt = 0 \n",
    "stanDev = 0.1\n",
    "\n",
    "weights = {\n",
    "    'wc1': tf.Variable(tf.truncated_normal([5, 5, 1, 6], mean=mean_wt, stddev=stanDev)),\n",
    "    'wc2': tf.Variable(tf.truncated_normal([5, 5, 6, 16], mean=mean_wt, stddev=stanDev)),\n",
    "    'fc1': tf.Variable(tf.truncated_normal([400, 120], mean_wt, stddev=stanDev)),\n",
    "    'fc2': tf.Variable(tf.truncated_normal([120, 84], mean_wt, stddev=stanDev)),\n",
    "    'out': tf.Variable(tf.truncated_normal([84, n_classes], mean_wt, stddev=stanDev))\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'bc1': tf.Variable(tf.zeros(6)),\n",
    "    'bc2': tf.Variable(tf.zeros(16)),\n",
    "    'bf1': tf.Variable(tf.zeros(120)),\n",
    "    'bf2': tf.Variable(tf.zeros(84)),\n",
    "    'out': tf.Variable(tf.zeros(n_classes))\n",
    "}\n",
    "\n",
    "strides ={\n",
    "    'conv1': [1,1,1,1],\n",
    "    'maxPool1':[1,2,2,1],\n",
    "    'conv2' : [1,1,1,1],\n",
    "    'maxPool2':[1,2,2,1]\n",
    "}\n",
    "\n",
    "filters ={\n",
    "    'maxPool1': [1,2,2,1],\n",
    "    'maxPool2': [1,2,2,1]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# MNIST consists of 28x28x1, grayscale images\n",
    "x = tf.placeholder(tf.float32, (None, 784))\n",
    "# Classify over 10 digits 0-9\n",
    "y = tf.placeholder(tf.float32, (None, 10))\n",
    "keep_prob = tf.placeholder(tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NETWORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def LeNet(x, weights, biases, strides, filters):\n",
    "    # from 2D (784,1) To 4D\n",
    "    x = tf.reshape(x,(-1,28,28,1))\n",
    "    # Add padding to width and height keeping 0 for other dimensions\n",
    "    x = tf.pad(x, [[0,0], [2,2], [2,2],[0,0]], mode=\"CONSTANT\")\n",
    "    ### Layer 1 ###\n",
    "    # convolution\n",
    "    conv_layer1 = tf.nn.conv2d(x, weights['wc1'], strides=strides['conv1'],padding='VALID') + biases['bc1']\n",
    "    # activation\n",
    "    conv_layer1 = tf.nn.relu(conv_layer1)\n",
    "    # pooling\n",
    "    conv_layer1 = tf.nn.max_pool(conv_layer1, ksize=filters['maxPool1'],strides=strides['maxPool1'],padding='VALID')\n",
    "    ### Layer 2 ###\n",
    "    # convolution\n",
    "    conv_layer2 = tf.nn.conv2d(conv_layer1, weights['wc2'], strides=strides['conv2'], padding='VALID') + biases['bc2']\n",
    "    # activation\n",
    "    conv_layer2 = tf.nn.relu(conv_layer2)\n",
    "    # pooling\n",
    "    conv_layer2 = tf.nn.max_pool(conv_layer2, ksize=filters['maxPool2'],strides=strides['maxPool2'],padding='VALID')\n",
    "    ### Layer 3 ###\n",
    "    # flatten\n",
    "    flatten = tf.contrib.layers.flatten(conv_layer2)\n",
    "    ### Layer 4 ###\n",
    "    # fully connected\n",
    "    fc1 = tf.matmul(flatten, weights['fc1']) + biases['bf1']\n",
    "    # activation\n",
    "    fc1 = tf.nn.relu(fc1)\n",
    "    ### Layer 5 ###\n",
    "    # fully connected\n",
    "    fc2 = tf.matmul(fc1, weights['fc2']) + biases['bf2']\n",
    "    # activation\n",
    "    fc2 = tf.nn.relu(fc2)\n",
    "    ### Layer 6 ###\n",
    "     # fully connected\n",
    "    logits = tf.matmul(fc2, weights['out']) + biases['out']\n",
    "    \n",
    "    return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = LeNet(x, weights, biases, strides, filters)\n",
    "\n",
    "# Define loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer()\n",
    "train_op = opt.minimize(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(X_data, y_data):\n",
    "    num_examples = len(X_data)\n",
    "    total_accuracy = 0\n",
    "    sess = tf.get_default_session()\n",
    "    for offset in range(0, num_examples, BATCH_SIZE):\n",
    "        batch_x, batch_y = X_data[offset:offset+BATCH_SIZE], y_data[offset:offset+BATCH_SIZE]\n",
    "        accuracy = sess.run(accuracy_operation, feed_dict={x: batch_x, y: batch_y})\n",
    "        total_accuracy += (accuracy * len(batch_x))\n",
    "    return total_accuracy / num_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def next_batch(num, data, labels):\n",
    "    '''\n",
    "    Return a total of `num` random samples and labels. \n",
    "    '''\n",
    "    idx = np.arange(0 , len(data))\n",
    "    np.random.shuffle(idx)\n",
    "    idx = idx[:num]\n",
    "    data_shuffle = [data[ i] for i in idx]\n",
    "    labels_shuffle = [labels[ i] for i in idx]\n",
    "\n",
    "    return np.asarray(data_shuffle), np.asarray(labels_shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2850"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features.shape[0]//batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on 142500 Samples...\n",
      "\n",
      "Epoch  1, Batch   1 -Loss:     2.2075 Validation Accuracy: 0.163867\n",
      "Epoch  1, Batch   2 -Loss:     2.1926 Validation Accuracy: 0.186133\n",
      "Epoch  1, Batch   3 -Loss:     2.2026 Validation Accuracy: 0.211067\n",
      "Epoch  1, Batch   4 -Loss:     2.2320 Validation Accuracy: 0.225200\n",
      "Epoch  1, Batch   5 -Loss:     2.1103 Validation Accuracy: 0.236533\n",
      "Epoch  1, Batch   6 -Loss:     2.1312 Validation Accuracy: 0.242000\n",
      "Epoch  1, Batch   7 -Loss:     2.1551 Validation Accuracy: 0.255467\n",
      "Epoch  1, Batch   8 -Loss:     2.0899 Validation Accuracy: 0.294533\n",
      "Epoch  1, Batch   9 -Loss:     2.1177 Validation Accuracy: 0.325733\n",
      "Epoch  1, Batch  10 -Loss:     1.9691 Validation Accuracy: 0.348400\n",
      "Epoch  1, Batch  11 -Loss:     2.0976 Validation Accuracy: 0.336800\n",
      "Epoch  1, Batch  12 -Loss:     1.9667 Validation Accuracy: 0.322667\n",
      "Epoch  1, Batch  13 -Loss:     1.9733 Validation Accuracy: 0.294800\n",
      "Epoch  1, Batch  14 -Loss:     1.9204 Validation Accuracy: 0.302000\n",
      "Epoch  1, Batch  15 -Loss:     1.8192 Validation Accuracy: 0.336533\n",
      "Epoch  1, Batch  16 -Loss:     1.8340 Validation Accuracy: 0.383333\n",
      "Epoch  1, Batch  17 -Loss:     1.8458 Validation Accuracy: 0.428933\n",
      "Epoch  1, Batch  18 -Loss:     1.7817 Validation Accuracy: 0.470400\n",
      "Epoch  1, Batch  19 -Loss:     1.6729 Validation Accuracy: 0.481867\n",
      "Epoch  1, Batch  20 -Loss:     1.6938 Validation Accuracy: 0.503867\n",
      "Epoch  1, Batch  21 -Loss:     1.6695 Validation Accuracy: 0.505200\n",
      "Epoch  1, Batch  22 -Loss:     1.5345 Validation Accuracy: 0.518800\n",
      "Epoch  1, Batch  23 -Loss:     1.5402 Validation Accuracy: 0.561867\n",
      "Epoch  1, Batch  24 -Loss:     1.5090 Validation Accuracy: 0.608533\n",
      "Epoch  1, Batch  25 -Loss:     1.4245 Validation Accuracy: 0.575067\n",
      "Epoch  1, Batch  26 -Loss:     1.3947 Validation Accuracy: 0.564667\n",
      "Epoch  1, Batch  27 -Loss:     1.3315 Validation Accuracy: 0.586933\n",
      "Epoch  1, Batch  28 -Loss:     1.3897 Validation Accuracy: 0.608400\n",
      "Epoch  1, Batch  29 -Loss:     1.2700 Validation Accuracy: 0.621200\n",
      "Epoch  1, Batch  30 -Loss:     1.0773 Validation Accuracy: 0.640400\n",
      "Epoch  1, Batch  31 -Loss:     1.4321 Validation Accuracy: 0.627867\n",
      "Epoch  1, Batch  32 -Loss:     1.2610 Validation Accuracy: 0.624800\n",
      "Epoch  1, Batch  33 -Loss:     1.2515 Validation Accuracy: 0.626533\n",
      "Epoch  1, Batch  34 -Loss:     1.0213 Validation Accuracy: 0.610933\n",
      "Epoch  1, Batch  35 -Loss:     0.9016 Validation Accuracy: 0.610267\n",
      "Epoch  1, Batch  36 -Loss:     1.2010 Validation Accuracy: 0.609333\n",
      "Epoch  1, Batch  37 -Loss:     1.0712 Validation Accuracy: 0.623733\n",
      "Epoch  1, Batch  38 -Loss:     1.2010 Validation Accuracy: 0.670533\n",
      "Epoch  1, Batch  39 -Loss:     1.2193 Validation Accuracy: 0.647200\n",
      "Epoch  1, Batch  40 -Loss:     0.7400 Validation Accuracy: 0.669867\n",
      "Epoch  1, Batch  41 -Loss:     1.2775 Validation Accuracy: 0.695600\n",
      "Epoch  1, Batch  42 -Loss:     1.0592 Validation Accuracy: 0.678800\n",
      "Epoch  1, Batch  43 -Loss:     0.9475 Validation Accuracy: 0.689067\n",
      "Epoch  1, Batch  44 -Loss:     0.9504 Validation Accuracy: 0.685200\n",
      "Epoch  1, Batch  45 -Loss:     0.7567 Validation Accuracy: 0.697867\n",
      "Epoch  1, Batch  46 -Loss:     1.2737 Validation Accuracy: 0.709067\n",
      "Epoch  1, Batch  47 -Loss:     0.6398 Validation Accuracy: 0.680400\n",
      "Epoch  1, Batch  48 -Loss:     1.2535 Validation Accuracy: 0.679467\n",
      "Epoch  1, Batch  49 -Loss:     1.0789 Validation Accuracy: 0.696800\n",
      "Epoch  1, Batch  50 -Loss:     0.8582 Validation Accuracy: 0.725600\n",
      "Epoch  1, Batch  51 -Loss:     0.7050 Validation Accuracy: 0.710933\n",
      "Epoch  1, Batch  52 -Loss:     0.9603 Validation Accuracy: 0.696133\n",
      "Epoch  1, Batch  53 -Loss:     1.1451 Validation Accuracy: 0.688267\n",
      "Epoch  1, Batch  54 -Loss:     0.9099 Validation Accuracy: 0.705467\n",
      "Epoch  1, Batch  55 -Loss:     1.0445 Validation Accuracy: 0.732133\n",
      "Epoch  1, Batch  56 -Loss:     0.9378 Validation Accuracy: 0.754533\n",
      "Epoch  1, Batch  57 -Loss:     0.9113 Validation Accuracy: 0.723200\n",
      "Epoch  1, Batch  58 -Loss:     0.8744 Validation Accuracy: 0.712667\n",
      "Epoch  1, Batch  59 -Loss:     0.7669 Validation Accuracy: 0.713200\n",
      "Epoch  1, Batch  60 -Loss:     0.7841 Validation Accuracy: 0.714667\n",
      "Epoch  1, Batch  61 -Loss:     0.6730 Validation Accuracy: 0.715867\n",
      "Epoch  1, Batch  62 -Loss:     1.2747 Validation Accuracy: 0.722400\n",
      "Epoch  1, Batch  63 -Loss:     0.8895 Validation Accuracy: 0.702933\n",
      "Epoch  1, Batch  64 -Loss:     0.6031 Validation Accuracy: 0.709733\n",
      "Epoch  1, Batch  65 -Loss:     0.8926 Validation Accuracy: 0.727200\n",
      "Epoch  1, Batch  66 -Loss:     0.7443 Validation Accuracy: 0.742667\n",
      "Epoch  1, Batch  67 -Loss:     0.6532 Validation Accuracy: 0.747200\n",
      "Epoch  1, Batch  68 -Loss:     0.7398 Validation Accuracy: 0.751467\n",
      "Epoch  1, Batch  69 -Loss:     0.7960 Validation Accuracy: 0.756267\n",
      "Epoch  1, Batch  70 -Loss:     1.0019 Validation Accuracy: 0.750667\n",
      "Epoch  1, Batch  71 -Loss:     1.2592 Validation Accuracy: 0.736667\n",
      "Epoch  1, Batch  72 -Loss:     0.7293 Validation Accuracy: 0.731467\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-86-04d64f5c8bef>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     26\u001b[0m                 \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mvalid_features\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m                 \u001b[0my\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mvalid_labels\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m                 keep_prob: 1.})\n\u001b[0m\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m             print('Epoch {:>2}, Batch {:>3} -'\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\image\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    787\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 789\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    790\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\image\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    995\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    996\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m--> 997\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    998\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    999\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\image\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1130\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1131\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m-> 1132\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m   1133\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\image\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1137\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1138\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1139\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1140\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1141\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\image\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1119\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[0;32m   1120\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1121\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m   1122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1123\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Initializing the variables\n",
    "init = tf. global_variables_initializer()\n",
    "\n",
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    num_examples = len(train_features)\n",
    "    \n",
    "    print('Training on', num_examples, 'Samples...')\n",
    "    print()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for batch in range(train_features.shape[0]//batch_size):\n",
    "            batch_x, batch_y = next_batch(batch_size, train_features, train_labels)\n",
    "            sess.run(train_op, feed_dict={\n",
    "                x: batch_x,\n",
    "                y: batch_y,\n",
    "                keep_prob: 0.5})\n",
    "\n",
    "            # Calculate batch loss and accuracy\n",
    "            loss = sess.run(cost, feed_dict={\n",
    "                x: batch_x,\n",
    "                y: batch_y,\n",
    "                keep_prob: 1.})\n",
    "            valid_acc = sess.run(accuracy, feed_dict={\n",
    "                x: valid_features,\n",
    "                y: valid_labels,\n",
    "                keep_prob: 1.})\n",
    "\n",
    "            print('Epoch {:>2}, Batch {:>3} -'\n",
    "                  'Loss: {:>10.4f} Validation Accuracy: {:.6f}'.format(\n",
    "                epoch + 1,\n",
    "                batch + 1,\n",
    "                loss,\n",
    "                valid_acc))\n",
    "\n",
    "    # Calculate Test Accuracy\n",
    "    test_acc = sess.run(accuracy, feed_dict={\n",
    "        x: test_features,\n",
    "        y: test_labels,\n",
    "        keep_prob: 1.})\n",
    "    print('Testing Accuracy: {}'.format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def eval_data(dataset):\n",
    "    \"\"\"\n",
    "    Given a dataset as input returns the loss and accuracy.\n",
    "    \"\"\"\n",
    "    # If dataset.num_examples is not divisible by BATCH_SIZE\n",
    "    # the remainder will be discarded.\n",
    "    # Ex: If BATCH_SIZE is 64 and training set has 55000 examples\n",
    "    # steps_per_epoch = 55000 // 64 = 859\n",
    "    # num_examples = 859 * 64 = 54976\n",
    "    #\n",
    "    # So in that case we go over 54976 examples instead of 55000.\n",
    "    steps_per_epoch = dataset.num_examples // BATCH_SIZE\n",
    "    num_examples = steps_per_epoch * BATCH_SIZE\n",
    "    total_acc, total_loss = 0, 0\n",
    "    sess = tf.get_default_session()\n",
    "    for step in range(steps_per_epoch):\n",
    "        batch_x, batch_y = dataset.next_batch(BATCH_SIZE)\n",
    "        loss, acc = sess.run([loss_op, accuracy_op], feed_dict={x: batch_x, y: batch_y})\n",
    "        total_acc += (acc * batch_x.shape[0])\n",
    "        total_loss += (loss * batch_x.shape[0])\n",
    "    return total_loss/num_examples, total_acc/num_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mnist' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-56-6a4e431d5f9a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m         \u001b[0msteps_per_epoch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmnist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_examples\u001b[0m \u001b[1;33m//\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m         \u001b[0mnum_examples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msteps_per_epoch\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'mnist' is not defined"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Load data\n",
    "    #mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        steps_per_epoch = mnist.train.num_examples // BATCH_SIZE\n",
    "        num_examples = steps_per_epoch * BATCH_SIZE\n",
    "\n",
    "        # Train model\n",
    "        for i in range(EPOCHS):\n",
    "            for step in range(steps_per_epoch):\n",
    "                batch_x, batch_y = mnist.train.next_batch(BATCH_SIZE)\n",
    "                loss = sess.run(train_op, feed_dict={x: batch_x, y: batch_y})\n",
    "\n",
    "            val_loss, val_acc = eval_data(mnist.validation)\n",
    "            print(\"EPOCH {} ...\".format(i+1))\n",
    "            print(\"Validation loss = {:.3f}\".format(val_loss))\n",
    "            print(\"Validation accuracy = {:.3f}\".format(val_acc))\n",
    "            print()\n",
    "\n",
    "        # Evaluate on the test data\n",
    "        test_loss, test_acc = eval_data(mnist.test)\n",
    "        print(\"Test loss = {:.3f}\".format(test_loss))\n",
    "        print(\"Test accuracy = {:.3f}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
